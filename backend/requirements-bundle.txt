fastapi==0.115.0
uvicorn[standard]==0.30.6
httpx==0.27.2

# Phase 1: Embedded Database Layer
aiosqlite==0.20.0
chromadb==0.6.3
kuzu==0.11.3
pydantic-settings==2.7.1

# Phase 2: PDF Ingestion Pipeline
PyMuPDF==1.27.1
python-multipart==0.0.20

# Phase 4: First-Run Setup / Model Download
huggingface_hub>=0.25.0
tqdm>=4.66.0

# Note: llama-cpp-python excluded from bundle â€” complex native build.
# Users should use Ollama for local LLM inference.
