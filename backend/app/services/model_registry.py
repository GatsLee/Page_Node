from app.models.setup import AvailableModel

MODEL_CATALOG: dict[str, AvailableModel] = {
    "llama31_8b": AvailableModel(
        id="llama31_8b",
        name="Llama 3.1 8B",
        repo_id="",
        filename="",
        param_count="8B",
        file_size_mb=0,
        ram_required_gb=6.0,
        description="Llama 3.1 8B via Ollama. No download needed if Ollama is running.",
        ollama_name="llama3.1:8b",
    ),
    "small": AvailableModel(
        id="small",
        name="Llama 3.2 1B",
        repo_id="bartowski/Llama-3.2-1B-Instruct-GGUF",
        filename="Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        param_count="1.2B",
        file_size_mb=808,
        ram_required_gb=2.0,
        description="Fast and lightweight. Good for basic concept extraction on any machine.",
        ollama_name="llama3.2:1b",
    ),
    "medium": AvailableModel(
        id="medium",
        name="Qwen 2.5 3B",
        repo_id="Qwen/Qwen2.5-3B-Instruct-GGUF",
        filename="qwen2.5-3b-instruct-q4_k_m.gguf",
        param_count="3B",
        file_size_mb=2100,
        ram_required_gb=4.0,
        description="Recommended. Strong instruction following and structured output quality.",
        recommended=True,
        ollama_name="qwen2.5:3b",
    ),
    "large": AvailableModel(
        id="large",
        name="Qwen 2.5 7B",
        repo_id="Qwen/Qwen2.5-7B-Instruct-GGUF",
        filename="qwen2.5-7b-instruct-q4_k_m.gguf",
        param_count="7B",
        file_size_mb=4680,
        ram_required_gb=8.0,
        description="Highest quality extraction. Needs 8+ GB free RAM.",
        ollama_name="qwen2.5:7b",
    ),
}
